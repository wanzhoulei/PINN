{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use ?\n",
    "In this section, I will walk you through the procedures of using PINN to solve 1d or 2d Poisson equations, either using full batch or mini-batch. I will fill in more technical and implementation details in later sections.\n",
    "### 2D Poisson\n",
    "All experiments that use PINN to solve 2D Poisson Eqaution are in the 2D_poisson folder. In this section, I will walk you through how to tune the hyperparameters and run all the experiments in the 2D_poisson folder.\n",
    "#### 1. Set the Truth Function and Its Laplacian\n",
    "Before building the Neural Network and doing any optimization, please define the truth function and its divergence. Please define the truth function in the beginning of PINN2D_tool.py. The user defined function should take two arguments x, y and return one value. It should be noted that this function should be able to act on numpy.ndarray array of shape (N, 2) in en element wise manner and returns a numpy array of shape (N,). This truth function is never used in the training process of any PINN. Rather, it is only used when plotting the truth solution as a comparison to the PINN solution. I provided four truth solution functions in the PINN2D_tool.py file: $u_1, u_2, u_3, u_4$. For example $u_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u1(x, y):\n",
    "    return np.sin( np.pi * x) * np.sin( np.pi * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that the function can act on a numpy.ndarray of shape (N, 2) elementwise. \n",
    "\n",
    "After defining the truth function, define the correspondant Laplacian. For example, if you defined $u_1$ as your intended truth solution, you need to define a function $div_1$ that is Laplacian of $u_1$:\n",
    "$$div_1 (x, y) = \\nabla^2 u_1(x, y) = u_{1xx}(x, y) + u_{1yy}(x, y)$$\n",
    "\n",
    "This function should take two inputs as the 2D coordinate of a data point and returns the Laplacian of the function defined before at this particular data point. Similarly, this function should also be able to take numpy.ndarray of shape (N, 2) as input and returns the elementwise operation of the Laplace operator on each data points. For example, after defining u1 above, we should define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the analytical laplacian of u1\n",
    "def div1(x, y):\n",
    "    return -2*np.pi**2*np.sin(np.pi*x)*np.sin(np.pi*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used in the training process of the PINN solving 2D Poisson Equation. In a real setting, this Laplacian is the only known. \n",
    "\n",
    "After defining the truth function and the Laplacian operator, you need to seed a few global variables in the PINN2D_tool.py file. Note that every time you change the truth function, you need to update these gloabl variables, otherwise the PINN will be solving another Poisson equation.\n",
    "\n",
    "Set the __func_RHS__ as the function handle of the truth function you defined. Set __div__ to be the function handle of the divergence of the function you defined. Set __min_val__ to be the minimum value of the truth function you defined. Set __max_val__ to be the maximum value of the truth function you defined. These two variables are only used for plotting purposes.\n",
    "\n",
    "For example, if you decide to use u1 as the truth function, you should define the global variables in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##change these two lines if you want to use other functions\n",
    "func_RHS = u1 ##set this variable to the function of true solution\n",
    "div = div1 ##set this variable to be the divergence of the above function \n",
    "min_val = -2; max_val=2 ##set the minimum and maximum values of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pre-Setting\n",
    "Before defining the PINN or doing any optimization, you need to set the following hyperparameters and variables in the experiment files. \n",
    "\n",
    "- n_iter: the number of iterations you wish to complete\n",
    "- alpha: damping factor for the kernel matrix (only applicable to natural gradient descent algorithms)\n",
    "- gamma: the weight you give to the boundary condition loss (will be covered in more detail later.)\n",
    "- write_pickle: if set to True, after the experiment, the PINN object will be serialized and saved into a .dat file, which can be read in another experiment to resume the previous experiment.If set to false, the object will not be saved after the experiment is finished.\n",
    "- read_pickle: Decides whether to resume a previous experiment. If you want to resume a previous experiment, please set this variable to be the link to the .dat file of the PINN object saved from the previous experiment. If you don't need to resume a previous experiment, please set this variable to None.\n",
    "- random_seed: The random seed you wish to use to initialize the parameters of the PINN.\n",
    "- N: set it to be the number of grid data points you wish to use in each dimention of the frame.\n",
    "- layers: numpy.ndarray object. Set it to your intended shape of the PINN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Running the Experiment\n",
    "Just type __python filename.py__ in the terminal to run the expriments after setting the hyperparameters. \n",
    "Our code provides the following Algorithm: $L^2$, $H^1$, $H^{-1}$, $\\dot{H}^1$, $\\dot{H}^{-1}$, Fisher-Rao, $W_2$ natural gradient descent, standard gradient descent, L-BFGS-B optimizer in the following files respectively:\n",
    "\n",
    "- GaussNewton_2dPoisson.py\n",
    "- H1_2dPoisson.py\n",
    "- Hinv_2dPoisson.py\n",
    "- H1semi_2dPoisson.py\n",
    "- Hinvsemi_2dPoisson.py\n",
    "- FR_2dPoisson.py\n",
    "- W2_2dPoisson.py\n",
    "- GD_2dPoisson.py\n",
    "- BFGS-2dPoisson.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN Solving 2D Poisson Equation Overview\n",
    "\n",
    "\n",
    "## True Solution\n",
    "You can set any truth solution, just define the function in the PINN2D_tool.py file. My code provides two functions to test. The first one has zero boundary condition, whereas the second has uniform boundary condition of 3. We use different truth solutions for different optimization methods: \n",
    "The solution of the function for standard gradient descent, Gauss Newton, $H^1$, $H^{-1}$, $\\dot{H}^1$, $\\dot{H}^{-1}$ natural gradient descent is:\n",
    "$$u(x, y) = sin(\\pi x)sin(\\pi y) + sin(3\\pi x)sin(3\\pi y)$$\n",
    "For Fisher-Rao and $W_2$ natural gradient descent, the truth function is:\n",
    "$$u(x, y) = sin(\\pi x)sin(\\pi y) + sin(3\\pi x)sin(3\\pi y)+ 3$$\n",
    "\n",
    "We used a different truth solution to impose a uniform boundary conditino of 3 because Fisher-Rao and $W_2$ natural gradient descent require that the function in this case $u$ is strictly positive everywhere. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Equation \n",
    "All 7 algorithms have the same 2D Poisson equation and the same RHS function f:\n",
    "$$u_{xx} + u_{yy} = f = -2\\pi^2sin(\\pi x)sin(\\pi y) - 18\\pi^2 sin(3\\pi x)sin(3\\pi y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Discretization\n",
    "The data frame we use is a square: $[-1, 1] \\times [-1, 1]$. We evenly discretize this frame into 50 by 50 data points. We have 196 boundary points and 2304 colocation points. The plot of the data points are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PINN2D_tool import *\n",
    "X_f_train, X_u_train, u_train = gridData(50)\n",
    "plotData(X_f_train, X_u_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the PINN is a numpy array of shape (2500, 2). We flattened this data points matrix into a 1d array, in which every element is a 2d coordinate of one data point. The PINN acts on this input array in an elementwise way. The output of the PINN will be a numpy array of shape (2500,), which is the evaluation of the PINN on each data points. \n",
    "\n",
    "The order we align the data points in the flattened 1d representation of the data points is important because it determines how we will compute the divergence and laplacian matrix. The way we flattened the data pints is columnwise, bottom to top and left to right. i.e. we iterate through all columns in the data matrix from left to right and in each column, we iterate each element from bottom to top. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Definition\n",
    "The loss function is created by a linear combination of the loss of colocation points and boundary points. Suppose the colocation loss is $loss_{f}$ and the boundary points loss is $loss_u$, the total loss is:\n",
    "$$loss = \\gamma  loss_u + (2-\\gamma)  loss_f$$\n",
    "\n",
    "### Colocation Loss $loss_f$\n",
    "Suppose we have N colocation points (i.e. interior points), the colocation loss is defined to be the MSE of the numerical Laplacian of the function $u$ at each colocation points and the real Laplacian $f$ defined in the Poisson Equation section above. Suppose $\\{ (x_i, y_i) \\}_{i=1}^N$ are the colocation points, then the colocation loss is:\n",
    "\n",
    "$$loss_f = \\frac{1}{N}\\sum_{i=1}^N || u_{xx}(x_i, y_i) + u_{yy}(x_i, y_i) - f(x_i, y_i)||^2$$\n",
    "\n",
    "The Laplacians of each colocation points are computed using the tensorflow.GradientTape method.\n",
    "\n",
    "### Boundary Condition Loss $loss_u$\n",
    "Suppose we have n boundary points, the boundary condition loss is defined to be the MSE of the evaluations of the PINN on boundary points and the real boundary value. Suppose the boundary truth value is B, then:\n",
    "\n",
    "$$loss_u  = \\frac{1}{n}\\sum_{i=1}^n || u(x_i, y_i) - B||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN Architecture\n",
    "We used PINN of shape (2, 20, 30, 20, 1). The activation functions are all tanh function. However, you can use any shape in the PINN, just input different layer shapes as a numpy array. \n",
    "\n",
    "### Parameter Initialization\n",
    "The weights of the PINN are initialized using Gaussian distributions. For weights $w$ between two layers, in which input dimension is $d_1$ and output dimension is $d_2$, $w$ satisfies:\n",
    "$$w \\sim N(0, \\frac{2}{d_1 + d_2})$$\n",
    "\n",
    "For Gauss Newton, $H^1$, $H^{-1}$, $\\dot{H}^1$, $\\dot{H}^{-1}$ natural gradient descent, the biases are all initialized to be 0. For Fisher-Rao and $w_2$ natural gradient descent, all biases are initialized to bw zero, except the biases in the last layer, which are inialized to be 3. \n",
    "\n",
    "We use a random seed to generate these weights parameters. Fixing the random seed, the initial parameter of the PINN are identity for all algorithms and experiments. But note that the biases in the last layer for Fisher-Rao and $W_2$ natural gradient descent are different from other algoroithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton-cg framework and Kernel Matrix\n",
    "For all the experiments in the 2D_poisson folder, we use scipy.optimize.minize's 'newton-cg' framework to conduct the standard gradient descent, $L_2$, $H^1$, $H^{-1}$, $\\dot{H}^1$, $\\dot{H}^{-1}$, Fisher-Rao, $W_2$ natural gradient descent.The documentation for this framework can be reviewed here: https://docs.scipy.org/doc/scipy/reference/optimize.minimize-newtoncg.html#optimize-minimize-newtoncg.\n",
    "\n",
    "Originally, this framework is for minimization of scalar function of one or more variables, using the Newton-CG algorithm. Basically, it takes a Hessian matrix H and computes the descent direction $\\eta$ such that:\n",
    "$$H\\eta = -\\frac{d\\rho}{d\\theta}$$\n",
    "where $\\rho$ is the scalar loss function and $\\theta$ is a vector of all the trainable parameters of the PINN, the RHS is the negative gradient of the loss function w.r.t. all the parameters of the PINN.\n",
    "\n",
    "The optimizer does not solve for the $\\eta$ directly, rather it uses a iterative numerical method to compute the descent direction. Then, instead of fixed step size, the framework optimizes the step size in each iteration and does the descent iteration using the computed descent direction and optimized step size.\n",
    "\n",
    "In our case, we expanded the usage of this framework by using the kernel matrix of many natural gradient descent as the input Hessian matrix. Using this framework, we do not need fixed step size because the step size of each iteration is optimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divergence and Negative Laplacian Operator Matrix\n",
    "Before diving into the kernel matrix of any natural gradient descent algorithm, first I will show you how I discretized the divergence operator and the negative laplacian operator, which are both important in computing some kernel matrices.\n",
    "\n",
    "#### Divergence Operator \n",
    "The discretized version of the Divergence Operator as a matrix is defined in the Cn method in the PINN2D_tool.py file. We used finite differentiation method to discretize the divergence operator. It should be noted that this discretized divergence operator assumes zero bounday condition in the truth function $u$, which is generally satisfied in all of our 2D poisson PINN examples except the ones using Fisher-Rao and $W_2$ natural gradient descent algorithm because they use uniform non-zero boundary condition (in our case 3). It can be a problem, but will only affect a small portion of the divergence of the entire data points, especially when the number of data points is large. But we need a new version of the divergence operator such that nothing is broken. We will update it in the future version.\n",
    "\n",
    "Suppose there are N data points in each dimension in the dataframe, i.e. $N^2$ data points in total and $(N-2)^2$ colocation points. The divergence matrix $C \\in \\mathbb{R}^{2(N-2)^2 \\times (N-2)^2}$ acts on a vector $\\vec{v} \\in \\mathbb{R}^{(N-2)^2}$, which is the evaluation of any 2D vector function $g$ (in our case the PINN function or the truth function u) evaluated at the $(N-2)^2$ colocation points and returns a vector in $\\mathbb{R}^{2(N-2)^2}$. The first half of this output vector is the gradient of $g$ w.r.t. x evaluated at all the colocation points and the second half of the output vector is the gradient of $g$ w.r.t. y evaluated at all the colocation points:\n",
    "\n",
    "$$C\\vec{v} = \\begin{bmatrix}\n",
    "g_x \\\\ \n",
    "g_y\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "In real implementation, we used finite differentiation to achieve this. \n",
    "\n",
    "For example, suppose $N=4$, then the discretized divergence operator $C \\in \\mathbb{R}^{8 \\times 4}$ and is constructed in the following way:\n",
    "\n",
    "1. Instantiate matrix $A = \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\-1 & 0 & 1 & 0 \\\\ 0 & -1 & 0 & 1 \\\\ 0 & 0 & -1 & 0\\end{bmatrix}$\n",
    "2. Compute $A_1 = \\frac{1}{2dx}I_4 \\bigotimes A$, where $\\bigotimes$ is the Kronecker product, $I_4$ is the identity matrix and $dx$ is the horizontal distance between each data points.\n",
    "3. Compute $A_2 = \\frac{1}{2dy}A \\bigotimes I_4$, where $dy$ is the vertical distance between each data points.\n",
    "4. Vertically concatenate $A_1, A_2$ to obatain $C = \\begin{bmatrix} A_1 \\\\ A_2 \\end{bmatrix}$\n",
    "\n",
    "#### Negative Laplace Operator\n",
    "The discretized negative Laplace Operator matrix is defined in the method NegLaplacian in PINN2D_tool.py. This method takes N, the number of grid points in each dimension as the input and returns the matrix $L \\in \\mathbb{R}^{N^2 \\times N^2}$, that is equivalent to the following operator:\n",
    "$$-\\nabla^2 f = - \\frac{\\partial{^2f}}{\\partial{x^2}} - \\frac{\\partial{^2f}}{\\partial{y^2}}$$\n",
    "where $f$ is 2D scalar function whose domain is discretized. Suppose $\\vec{v} \\in \\mathbb{R}^{N^2}$ is the evaluation of function $f$ over all the discretized data points in the frame and the order is the same as the order described before, then $L$ acting on $\\vec{v}$ returns a vector in $\\mathbb{R}^{N^2}$ of which each element is the negative laplacian of the function $f$ evaluated at each data point.\n",
    "\n",
    "In real implementation, we first compute the divergence operator matrix $C$. Then, we construct an identity matrix of size $\\mathbb{R}^{N^2 \\times N^2}$. For the columns and rows that correspond to the colocation points, we copy $C^T C$ to it and get the resulting matrix as $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Gradient Descent\n",
    "To achieve standard gradient, we set $H = I$ indentity matrix, in the newton-cg framework such that:\n",
    "\n",
    "$$\\eta = -\\frac{d\\rho}{d\\theta}$$\n",
    "\n",
    "#### $L^2$ Natural Gradient Descent\n",
    "The kernel matrix of $L^2$ natural gradient descent is:\n",
    "$$G_{L^2} = \\alpha I + J^TJ$$\n",
    "where $J$ is the Jacobian of the PINN function w.r.t. all parameters. $\\alpha$ is the damping factor of the kernel. By default, damping factor is set to 0. In occation that there are more parameters then data points, $J^T J $ will be singular and it is preferable to add a non-zero damping factor such that the kernel matrix can be inverted.\n",
    "\n",
    "We input this kernel $G_{L^2}$ to the newton-cg framework as the $H$ matrix such that the following equation is solved iteratively for the $L^2$ descent direction:\n",
    "\n",
    "$$G_{L^2} \\eta^{L^2}_{nat} = -\\frac{\\partial \\rho}{\\partial \\theta}$$\n",
    "\n",
    "The $L^2$ natural gradient descent is then performed with optimized step size.\n",
    "\n",
    "#### $H^1$ Natural Gradient Descent \n",
    "The kernel matrix of $H^1$ natural gradient descent is:\n",
    "$$G_{H^1} = \\alpha I + J^T (I + L) J$$\n",
    "where $J$ is the Jacobian of the PINN function w.r.t. all parameters. $L$ is the diecretized negative divergence operator matrix. $\\alpha$ is the damping factor. By default, damping factor is set to 0. In occation that there are more parameters then data points, the kernel will be singular and it is preferable to add a non-zero damping factor such that the kernel matrix can be inverted.\n",
    "\n",
    "We input this kernel $G_{H^1}$ to the newton-cg framework as the $H$ matrix such that the following equation is solved iteratively for the $H^1$ descent direction:\n",
    "\n",
    "$$G_{H^1} \\eta^{H^1}_{nat} = -\\frac{\\partial \\rho}{\\partial \\theta}$$\n",
    "\n",
    "The $H^1$ natural gradient descent is then performed with optimized step size.\n",
    "\n",
    "#### $\\dot{H}^1$ Natural Gradient Descent\n",
    "The kernel matrix of $\\dot{H}^1$ natural gradient descent is:\n",
    "$$G_{\\dot{H}^1} = \\alpha I + J^T L J$$\n",
    "where $J$ is the Jacobian of the PINN function w.r.t. all parameters. $L$ is the diecretized negative divergence operator matrix. $\\alpha$ is the damping factor. By default, damping factor is set to 0. In occation that there are more parameters then data points, the kernel will be singular and it is preferable to add a non-zero damping factor such that the kernel matrix can be inverted.\n",
    "\n",
    "We input this kernel $G_{\\dot{H}^1}$ to the newton-cg framework as the $H$ matrix such that the following equation is solved iteratively for the $\\dot{H}^1$ descent direction:\n",
    "\n",
    "$$G_{\\dot{H}^1} \\eta^{\\dot{H}^1}_{nat} = -\\frac{\\partial \\rho}{\\partial \\theta}$$\n",
    "\n",
    "The $\\dot{H}^1$ natural gradient descent is then performed with optimized step size.\n",
    "\n",
    "#### $H^{-1}$ Natural Gradient Descent \n",
    "The kernel matrix of $H^{-1}$ natural gradient descent is:\n",
    "$$G_{H^{-1}} = \\alpha I + J^T (I + L)^{-1} J$$\n",
    "where $J$ is the Jacobian of the PINN function w.r.t. all parameters. $L$ is the diecretized negative divergence operator matrix. $\\alpha$ is the damping factor. By default, damping factor is set to 0. In occation that there are more parameters then data points, the kernel will be singular and it is preferable to add a non-zero damping factor such that the kernel matrix can be inverted.\n",
    "\n",
    "We input this kernel $G_{H^{-1}}$ to the newton-cg framework as the $H$ matrix such that the following equation is solved iteratively for the $H^{-1}$ descent direction:\n",
    "\n",
    "$$G_{H^{-1}} \\eta^{H^{-1}}_{nat} = -\\frac{\\partial \\rho}{\\partial \\theta}$$\n",
    "\n",
    "The $H^{-1}$ natural gradient descent is then performed with optimized step size.\n",
    "\n",
    "#### $\\dot{H}^{-1}$ Natural Gradient Descent\n",
    "The kernel matrix of $\\dot{H}^{-1}$ natural gradient descent is:\n",
    "$$G_{\\dot{H}^{-1}} = \\alpha I + J^T L^{-1} J$$\n",
    "where $J$ is the Jacobian of the PINN function w.r.t. all parameters. $L$ is the diecretized negative divergence operator matrix. $\\alpha$ is the damping factor. By default, damping factor is set to 0. In occation that there are more parameters then data points, the kernel will be singular and it is preferable to add a non-zero damping factor such that the kernel matrix can be inverted.\n",
    "\n",
    "We input this kernel $G_{\\dot{H}^{-1}}$ to the newton-cg framework as the $H$ matrix such that the following equation is solved iteratively for the $\\dot{H}^{-1}$ descent direction:\n",
    "\n",
    "$$G_{\\dot{H}^{-1}} \\eta^{\\dot{H}^{-1}}_{nat} = -\\frac{\\partial \\rho}{\\partial \\theta}$$\n",
    "\n",
    "The $\\dot{H}^{-1}$ natural gradient descent is then performed with optimized step size.\n",
    "\n",
    "#### Fisher-Rao Natural Gradient Descent\n",
    "The kernel matrix of Fisher-Rao Natural gradient descent is:\n",
    "$$G_{FR} = \\alpha I + J^T \\Phi J$$\n",
    "\n",
    "$J \\in \\mathbb{R}^{N^2 \\times p}$ where $N^2$ is the total number of data points and p is the number of all parameters of PINN. $\\Phi \\in \\mathbb{R}^{N^2 \\times N^2}$ is a diagonal matrix whose diagonal is the inverse of the evaluations of the PINN on the $N^2$ data points. $\\alpha$ is the damping factor. By default, damping factor is set to 0. In occation that there are more parameters then data points, the kernel will be singular and it is preferable to add a non-zero damping factor such that the kernel matrix can be inverted.\n",
    "\n",
    "We input this kernel $G_{FR}$ to the newton-cg framework as the $H$ matrix such that the following equation is solved iteratively for the Fisher-Rao descent direction:\n",
    "\n",
    "$$G_{FR} \\eta^{FR}_{nat} = -\\frac{\\partial \\rho}{\\partial \\theta}$$\n",
    "\n",
    "The Fisher-Rao natural gradient descent is then performed with optimized step size.\n",
    "\n",
    "#### $W^2$ Natural Gradient Descent\n",
    "The kernel matrix of Wasserstein Natural Gradient Descent is:\n",
    "$$G_{W^2} = \\alpha I + J^T(C^T U C)^{-1} J$$\n",
    "$J$ is the Jacobian of the PINN over all colocation points w.r.t. all parameters of PINN. $J \\in \\mathbb{R}^{(N-2)^2 \\times p}$, where p is the number of parameters of PINN and N is the number of data points in each dimention of the discretized frame. The matrix $C \\in \\mathbb{R}^{2(N-2)^2 \\times (N-2)^2}$ is the matrix of discretized divergence operator defined in the Cn method $U \\in \\mathbb{R}^{2(N-2)^2 \\times 2(N-2)^2}$ is a diagonal matrix whose diagonal is the evaluation of the PINN over all colocation points repeating twice in the same order. \n",
    "\n",
    "$\\alpha$ is the damping factor. By default, damping factor is set to 0. In occation that there are more parameters then data points, the kernel will be singular and it is preferable to add a non-zero damping factor such that the kernel matrix can be inverted.\n",
    "\n",
    "We input this kernel $G_{W^2}$ to the newton-cg framework as the $H$ matrix such that the following equation is solved iteratively for the Fisher-Rao descent direction:\n",
    "\n",
    "$$G_{W^2} \\eta^{W^2}_{nat} = -\\frac{\\partial \\rho}{\\partial \\theta}$$\n",
    "\n",
    "The Wassertein natural gradient descent is then performed with optimized step size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "#### Weight $\\gamma$ in loss function\n",
    "The loss function is created by a linear combination of the loss of colocation points and boundary points. Suppose the colocation loss is $loss_{f}$ and the boundary points loss is $loss_u$, the total loss is:\n",
    "$$loss = \\gamma  loss_u + (2-\\gamma)  loss_f$$\n",
    "Since there is a magnitude separation between the boundary loss and the colocation loss, we use $\\gamma = 1.99$ to add more weights on the boundary points to impose the boudnary condition.\n",
    "\n",
    "#### Number of iterations\n",
    "We did 2000 iterations for all 7 algorithms.\n",
    "\n",
    "#### Damping factor\n",
    "We used damping factor $\\alpha=0$ for all natural gradient descent algorithms.\n",
    "\n",
    "#### Random Seed\n",
    "We use random seed 0 to generate the initial parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
